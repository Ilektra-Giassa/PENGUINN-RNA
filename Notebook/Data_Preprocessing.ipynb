{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "** This code can be executed in google colab **\n",
        "\n",
        "Content ready for pre-processing of the RNA Gquadruplexes bed file.\n",
        "\n",
        " The datasets can be downloaded from ' https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE77282 ' reference\n",
        " \n",
        "\n"
      ],
      "metadata": {
        "id": "BHVRqONPV0Zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading GEO Datasets\n",
        "!wget https://ftp.ncbi.nlm.nih.gov/geo/series/GSE77nnn/GSE77282/suppl/GSE77282_K_hits.bed.gz\n",
        "!wget https://ftp.ncbi.nlm.nih.gov/geo/series/GSE77nnn/GSE77282/suppl/GSE77282_KPDS_hits.bed.gz\n",
        "!wget https://ftp.ncbi.nlm.nih.gov/refseq/H_sapiens/annotation/GRCh37_latest/refseq_identifiers/GRCh37_latest_genomic.fna.gz"
      ],
      "metadata": {
        "id": "YpozPs3b3f84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ef317a5-9392-49d5-a630-c8a895bcbe3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-18 13:56:50--  https://ftp.ncbi.nlm.nih.gov/geo/series/GSE77nnn/GSE77282/suppl/GSE77282_K_hits.bed.gz\n",
            "Resolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 165.112.9.228, 165.112.9.229, 2607:f220:41f:250::230, ...\n",
            "Connecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|165.112.9.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 138515 (135K) [application/x-gzip]\n",
            "Saving to: ‘GSE77282_K_hits.bed.gz’\n",
            "\n",
            "GSE77282_K_hits.bed 100%[===================>] 135.27K   248KB/s    in 0.5s    \n",
            "\n",
            "2022-05-18 13:56:53 (248 KB/s) - ‘GSE77282_K_hits.bed.gz’ saved [138515/138515]\n",
            "\n",
            "--2022-05-18 13:56:53--  https://ftp.ncbi.nlm.nih.gov/geo/series/GSE77nnn/GSE77282/suppl/GSE77282_KPDS_hits.bed.gz\n",
            "Resolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 165.112.9.228, 165.112.9.229, 2607:f220:41f:250::230, ...\n",
            "Connecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|165.112.9.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 441833 (431K) [application/x-gzip]\n",
            "Saving to: ‘GSE77282_KPDS_hits.bed.gz’\n",
            "\n",
            "GSE77282_KPDS_hits. 100%[===================>] 431.48K   592KB/s    in 0.7s    \n",
            "\n",
            "2022-05-18 13:56:55 (592 KB/s) - ‘GSE77282_KPDS_hits.bed.gz’ saved [441833/441833]\n",
            "\n",
            "--2022-05-18 13:56:55--  https://ftp.ncbi.nlm.nih.gov/refseq/H_sapiens/annotation/GRCh37_latest/refseq_identifiers/GRCh37_latest_genomic.fna.gz\n",
            "Resolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 165.112.9.228, 165.112.9.229, 2607:f220:41f:250::230, ...\n",
            "Connecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|165.112.9.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 943912841 (900M) [application/x-gzip]\n",
            "Saving to: ‘GRCh37_latest_genomic.fna.gz’\n",
            "\n",
            "GRCh37_latest_genom 100%[===================>] 900.18M  16.1MB/s    in 57s     \n",
            "\n",
            "2022-05-18 13:57:53 (15.8 MB/s) - ‘GRCh37_latest_genomic.fna.gz’ saved [943912841/943912841]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install pybedtools\n",
        "!apt-get install bedtools\n",
        "!pip install pybedtools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh1VVxxpYFw-",
        "outputId": "14202225-c671-4f0b-886e-b72307c1e6c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "bedtools is already the newest version (2.26.0+dfsg-5).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libnvidia-common-460 nsight-compute-2020.2.0\n",
            "Use 'apt autoremove' to remove them.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 67 not upgraded.\n",
            "Requirement already satisfied: pybedtools in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: pysam in /usr/local/lib/python3.7/dist-packages (from pybedtools) (0.19.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pybedtools) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decompress files\n",
        "!gunzip -rk GRCh37.p13.genome.fa.gz\n",
        "!gunzip GSE77282_KPDS_hits.bed.gz\n",
        "!gunzip GSE77282_K_hits.bed.gz\n"
      ],
      "metadata": {
        "id": "a8oJXpxx6Tyi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d46b3cc5-bc38-425c-fb78-f34606e8b470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gzip: GRCh37.p13.genome.fa.gz: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Reading the file \n",
        "df1= pd.read_csv(\"GSE77282_K_hits.bed\", sep='\\t',header=None)\n",
        "df2=pd.read_csv(\"GSE77282_KPDS_hits.bed\",sep='\\t', header=None)\n",
        "K_hits= df1[[0,1,2,3,4,5]] \n",
        "KPDS_hits = df2[[0,1,2,3,4,5]] \n",
        "\n",
        "#Selecting the required cols and restoring to new files \n",
        "\n",
        "K_hits.to_csv(\"K_hits.csv\", index=False, sep='\\t',header=False)\n",
        "KPDS_hits.to_csv(\"KPDS_hits.csv\", index=False, sep='\\t', header=False)\n"
      ],
      "metadata": {
        "id": "WtqU0NuKUE_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided datasets are of various length, we are here trying to unify the length of the reads upto 200 ntd. \n",
        "\n",
        "\n",
        ">  The sequences are extended if we have reads shorter than 200 ntd with the flanking transcripts\n",
        "\n",
        "\n",
        "\n",
        "> If Sequences are longer than 200 nt, those read are centered and cut to 200 nt\n",
        "\n",
        "\n",
        "\n",
        ">The sequence of the size 200 were consider same\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QfK7dKGYBAkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extending sequences shorter than 200 nucleotides\n",
        "\n",
        "import pandas as pd\n",
        "import glob, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import sys\n",
        "#os.chdir(\"/content/\")\n",
        "\n",
        "fixed_len=200\n",
        "df = pd.read_csv('KPDS_hits.csv', sep='\\t', comment='t', header=None)\n",
        "df.columns = ['chrom', 'chromStart', 'chromEnd', 'name', 'score', 'strand']\n",
        "print(df)\n",
        "chr_start=df.chromStart\n",
        "chr_end=df.chromEnd\n",
        "len_diff=chr_end-chr_start\n",
        "ext_chrstr= chr_start -(fixed_len-len_diff)/2\n",
        "ext_chrend= chr_end +(fixed_len-len_diff)/2\n",
        "team=pd.DataFrame(list(zip(ext_chrstr,ext_chrend)))\n",
        "team.columns=['ext_chromStart','ext_chromEnd'] \n",
        "df['chromStart'] = team['ext_chromStart']\n",
        "df['chromEnd'] = team['ext_chromEnd']\n",
        "\n",
        "df.to_csv('KPDS_out.bed', sep='\\t', index=False, header=False)"
      ],
      "metadata": {
        "id": "NqQ3kF5SVCdF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a308e8a-cfe3-478e-fded-37d9b4b08a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      chrom  chromStart   chromEnd       name     score strand\n",
            "0      chr1      762160     762190  LINC00115  0.295290      -\n",
            "1      chr1      892334     892364      NOC2L  0.297222      -\n",
            "2      chr1      892371     892401      NOC2L  0.488479      -\n",
            "3      chr1      898579     898609     KLHL17  0.635253      +\n",
            "4      chr1      900936     900966     KLHL17  0.674253      +\n",
            "...     ...         ...        ...        ...       ...    ...\n",
            "11362  chrX   154299805  154299835      BRCC3  0.308929      +\n",
            "11363  chrX   154351100  154351130      BRCC3  0.853233      +\n",
            "11364  chrX   154719703  154719733      TMLHE  0.944949      -\n",
            "11365  chrX   154719710  154719740      TMLHE  0.914683      -\n",
            "11366  chrX   154842552  154842582      TMLHE  0.678597      -\n",
            "\n",
            "[11367 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pipeline run on Command Line Interface for extracting fasta files from bed files**\n",
        "\n",
        "---\n",
        "\n",
        "setting up conda environment, make sure you are in the right path and have all files in the same directory\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Indexing** of the downloaded Human Reference genome using samtools\n",
        "\n",
        "\n",
        "\n",
        "> samtools faidx GRCh37.p13.genome.fa\n",
        "\n",
        "\n",
        "\n",
        "# **fetching fasta files from Bed file format**\n",
        "\n",
        "\n",
        "> bedtools getfasta -s -fo KPDS_hits.fasta -name -fi GRCh37.p13.genome.fa -bed KPDS_hits.csv\n",
        "\n",
        "> bedtools getfasta -s -fo K_hits.fasta -name -fi GRCh37.p13.genome.fa -bed K_hits.csv\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YMJd-Y2O6wEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "#All sequences which were longer than 200 nt were centered and cut to the length of 200 nt.\n",
        "#Reads of 200 nts were kept as it is\n",
        "# %pip install pysam\n",
        "# %pip install biopython\n",
        "from pysam import FastxFile\n",
        "import random\n",
        "import math\n",
        "from Bio import SeqIO\n",
        "from Bio.Seq import Seq\n",
        "\n",
        "\n",
        "fasta_q_file = \"KPDS_hits.fasta\"\n",
        "out_filename = \"KPDS_hits_out.fasta\"\n",
        "size_trim = 200\n",
        "\n",
        "\n",
        "with FastxFile(fasta_q_file) as fh, open(out_filename, mode='w') as fout:\n",
        "\n",
        "  for entry in fh:\n",
        "    sequence_id = entry.name\n",
        "    sequence = str(entry.sequence)\n",
        "    seq_length = len(sequence)\n",
        "    if seq_length > size_trim:\n",
        "      start_range=math.floor((seq_length-size_trim)/2)\n",
        "      end_range=start_range+size_trim\n",
        "      new_seq=sequence[math.floor(start_range):math.ceil(end_range)]\n",
        "      fout.write(\">{}\\n{}\\n\".format(sequence_id, new_seq))\n",
        "      \n",
        "    else:\n",
        "      if seq_length==size_trim :\n",
        "        new_seq=sequence\n",
        "        fout.write(\">{}\\n{}\\n\".format(sequence_id, sequence))\n",
        "\n",
        "    continue"
      ],
      "metadata": {
        "id": "jkWDbeXeiHdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "#All sequences which were longer than 200 nt were centered and cut to the length of 200 nt.\n",
        "#Shorter sequences were randomly padded from both sides with Ns to become 200 nt long.\n",
        "#Reads of 200 nts were kept as it is\n",
        "# %pip install pysam\n",
        "# %pip install biopython\n",
        "from pysam import FastxFile\n",
        "import random\n",
        "from Bio import SeqIO\n",
        "from Bio.Seq import Seq\n",
        "\n",
        "#from pysam import FastxFile\n",
        "\n",
        "#fasta_q_file = \"K_hits.fasta\"\n",
        "#out_filename = \"K_hits_padded.fasta\"\n",
        "size_trim = 200\n",
        "\n",
        "\n",
        "with FastxFile(fasta_q_file) as fh, open(out_filename, mode='w') as fout:\n",
        "    print(fh)\n",
        "    for entry in fh:\n",
        "        sequence_id = entry.name\n",
        "        sequence = str(entry.sequence)\n",
        "        seq_length = len(sequence)\n",
        "        if seq_length > size_trim:\n",
        "          x=(seq_length-size_trim)/2\n",
        "          \n",
        "          y=seq_length-(x-1)\n",
        "          new_seq=sequence[int(x):int(y)]\n",
        "          \n",
        "         \n",
        "          fout.write(\">{}\\n{}\\n\".format(sequence_id, new_seq))\n",
        "        else:\n",
        "          if seq_length < size_trim :\n",
        "            x=(size_trim-seq_length)/2\n",
        "            y= \"N\"*int(x)\n",
        "            new_seq=y+sequence+y \n",
        "            \n",
        "            fout.write(\">{}\\n{}\\n\".format(sequence_id, new_seq))\n",
        "          else:\n",
        "            if seq_length==trim_size :\n",
        "              fout.write(\">{}\\n{}\\n\".format(sequence_id, sequence))\n",
        "\n",
        "            \n",
        "            continue"
      ],
      "metadata": {
        "id": "Tu_gLgERaaX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing fasta header and the positive files are ready**\n",
        "\n",
        "sed '/^>/d' K_hits_padded.fasta > K_hits_padded_header.fasta\n",
        "\n",
        "sed '/^>/d' KPDS_hits_padded.fasta > KPDS_hits_padded_header.fasta\n",
        "\n",
        "**Extracting Negative datasets**\n",
        "\n",
        "\n",
        "We used bedtools shuffle for suffling the reads over transcripts which were not from the positive sets.\n",
        "\n",
        "\n",
        "> The option -incl bed file contains all the transcripts with Exons,CDS,5UTR and 3UTR regions and the transcripts less than 300 bases were discarded.\n",
        "\n",
        "\n",
        "\n",
        "> Those positive which were not considered were excluded from the input with -excl options\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#bedtools suffle\n",
        "bedtools shuffle -i  K_positive.bed -g GRCh37.p13.genome -incl GRCh37.p13_gtf-l300.bed -excl  K_positive.bed > K_shuffled_negative.bed\n",
        "bedtools shuffle -i KPDS_positive.bed -g GRCh37.p13.genome -incl GRCh37.p13_gtf-l300.bed -excl K_new_negative.bed > KPDS_shuffled_negative.bed\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vgvC6JrpFd1N"
      }
    }
  ]
}